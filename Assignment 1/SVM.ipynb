{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T90cX4ichklP"
   },
   "source": [
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "One thing SVMs are very good at is text classification.\n",
    "\n",
    "The goal here is to determine whether a tweet was written by a Democratic or Republican politician, using just the text of the tweet.\n",
    "\n",
    "`sklearn` library is used in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICj3ClLyhklU"
   },
   "source": [
    "The data has three fields:\n",
    "\n",
    "| Feature \t| Description               \t|\n",
    "|---------\t|---------------------------\t|\n",
    "|   Party \t| Democrat or Republican    \t|\n",
    "|  Handle \t| The author's Twitter name \t|\n",
    "|   Tweet \t| The text of the tweet     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L6UDOfFbhklV",
    "outputId": "d578372b-fed8-4004-ec5e-72ee78b079c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepStephMurphy</td>\n",
       "      <td>RT @SSNAlerts: .@RepStephMurphy's, Mike Kelly'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39638</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>cbrangel</td>\n",
       "      <td>America is greater thx to contributions of Lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44277</th>\n",
       "      <td>Republican</td>\n",
       "      <td>RepPaulMitchell</td>\n",
       "      <td>Speaking to @therealnmma this morning about my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70181</th>\n",
       "      <td>Republican</td>\n",
       "      <td>RepDavid</td>\n",
       "      <td>RT @DodieLondenEIPS: Congratulations to Britta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32440</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Republicans control the House, Senate and Whit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Party           Handle  \\\n",
       "4098     Democrat   RepStephMurphy   \n",
       "39638    Democrat         cbrangel   \n",
       "44277  Republican  RepPaulMitchell   \n",
       "70181  Republican         RepDavid   \n",
       "32440    Democrat  BennieGThompson   \n",
       "\n",
       "                                                   Tweet  \n",
       "4098   RT @SSNAlerts: .@RepStephMurphy's, Mike Kelly'...  \n",
       "39638  America is greater thx to contributions of Lat...  \n",
       "44277  Speaking to @therealnmma this morning about my...  \n",
       "70181  RT @DodieLondenEIPS: Congratulations to Britta...  \n",
       "32440  Republicans control the House, Senate and Whit...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/tweets.csv')\n",
    "\n",
    "# Training an SVM on a lot of data with a lot of features can take a few minutes,\n",
    "# so to keep things speedy here we will use a subset of the data.\n",
    "data = data.sample(5000, random_state=5)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e75pKcI0hklW"
   },
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agrR_Q1ThklW"
   },
   "source": [
    "How many politicians do we have tweets from, per party?\n",
    "\n",
    "(Not how many tweets per party!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PFYvrmmhklW",
    "outputId": "b42c8be8-714b-4435-d63b-6b6fa56f182b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Republican    222\n",
       "Democrat      211\n",
       "Name: Party, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset='Handle')['Party'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn55DQqQhklX"
   },
   "source": [
    "And how many tweets per politician?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdOWdL_RhklX",
    "outputId": "f44a620d-3aec-42c5-9da1-9b7cc943e63e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Handle\n",
       "AGBecerra          16\n",
       "AlanGrayson        18\n",
       "AnthonyBrownMD4     5\n",
       "AustinScottGA08     9\n",
       "BennieGThompson     6\n",
       "                   ..\n",
       "reppittenger       15\n",
       "repsandylevin      11\n",
       "rosadelauro        12\n",
       "sethmoulton         9\n",
       "virginiafoxx        7\n",
       "Name: Tweet, Length: 433, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Handle')['Tweet'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piOmrVcShklY"
   },
   "source": [
    "## Working with text data\n",
    "\n",
    "The features for an SVM can't be words or whole tweets. We need a numerical representation for the words in the texts. One method is to transform the text into TF-IDF vectors.\n",
    "\n",
    "It will take the tweets, tokenise them into words (using a special tokeniser that knows how best to split up tweets), remove stop words (very common words like \"the\" and \"and\", which do not really contribute to the meaning of a tweet much) then it will create a sparse matrix representation of all the tweets. Each row is a single tweet, each column is a word in the vocabulary of all the tweets.\n",
    "\n",
    "It only uses the 5000 most common words - using all ~200k words would take a long time to train a model.\n",
    "\n",
    "(This will take a few seconds!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km_Np6Z3evuW",
    "outputId": "ccf051ea-8edc-46af-d17b-90b4e30f3baa"
   },
   "outputs": [],
   "source": [
    "# you need to install twokenize. You can run this cell only once\n",
    "#!pip install twokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glRRmyW4hklY",
    "outputId": "c4f6c40d-aa1f-401d-9c7c-99eb46899408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 53445 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import twokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "transformer = TfidfVectorizer(tokenizer=twokenize.tokenizeRawTweetText, stop_words='english', max_features=5000)\n",
    "tweet_vecs = transformer.fit_transform(data['Tweet'])\n",
    "\n",
    "tweet_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvC_PfIOhklZ",
    "outputId": "5374e6c7-c02b-4f30-87a4-61344c3f0322"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rt', 4015),\n",
       " (':', 503),\n",
       " ('.', 325),\n",
       " (',', 318),\n",
       " ('mike', 3219),\n",
       " ('cracking', 1671),\n",
       " ('house', 2625),\n",
       " ('â€¦', 4977),\n",
       " ('america', 985),\n",
       " ('greater', 2422)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some words in the vocabulary and their IDs\n",
    "\n",
    "list(transformer.vocabulary_.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ynW7uv7hklZ"
   },
   "source": [
    "## Setting up the data\n",
    "\n",
    "As is standard, we will use some of our data for training the model and some of it for evaluating it. This gives a better idea of how well the model can generalise to unseen data, rather than simply overfitting to the data it has seen.\n",
    "\n",
    "First, we set up a variable `y` to store the Party labels we want to predict.\n",
    "\n",
    "Then, we use the `train_test_split` function to split up the `tweet_vecs` and the `y` data into train/test portions, using an 80:20 train:test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gf3xtiYjhkla"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['Party']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweet_vecs, y, test_size=0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i-l5eQ9hklb"
   },
   "source": [
    "## Task 1: Train a linear kernel SVM\n",
    "\n",
    "SVMs in `sklearn` have a few configurable options. The key ones are the kernel to be used (which can overcome non-separable data classes) and the regularization value $C$ (to relax or tighten the margins).\n",
    "\n",
    "Use a `for` loop to try different values for the kernel: `['linear', 'rbf', 'poly', 'sigmoid']`\n",
    "\n",
    "On each iteration, create a classifier with that `kernel`, and call `.fit()` with the training data.\n",
    "\n",
    "Then, use the `.score()` method to see how well the model did on both the seen and the unseen data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSDFtighhklb",
    "outputId": "b3e2041b-ee08-4040-b49c-6db49cd487de",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score on test data is 0.684 and on training data is 0.91025 using linear kernel\n",
      "The score on test data is 0.68 and on training data is 0.9905 using rbf kernel\n",
      "The score on test data is 0.665 and on training data is 0.99625 using poly kernel\n",
      "The score on test data is 0.689 and on training data is 0.8425 using sigmoid kernel\n",
      "Score on training data is obviously higher than the score on test data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Your code here...\n",
    "for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "    model = SVC(C = 1, kernel = kernel, random_state = 5).fit(X_train, y_train)\n",
    "    print(f\"The score on test data is {model.score(X_test, y_test)} and on training data is {model.score(X_train, y_train)} using {kernel} kernel\")\n",
    "print(\"Score on training data is obviously higher than the score on test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2dVcl9Fhklb"
   },
   "source": [
    "## Task 2: Find the best model parameters\n",
    "\n",
    "In addition to testing different kernels, try different values for $C$, the regularization hyperparameter.\n",
    "\n",
    "Rather than doing this in a loop, one model at a time, we can parallelise it using `GridSearchCV` in `sklearn`. \n",
    "\n",
    "The `GridSearchCV` class takes a model, with a dictionary of hyperparameters and values. Then you just fit/train it as usual, using the training data from before.\n",
    "\n",
    "Try the following:\n",
    "\n",
    "1. Different kernels\n",
    "2. A few values for $C$\n",
    "\n",
    "Below, create a `GridSearchCV` in the same way you would do with a model: assign it to a variable named `gcv`, pass it the `classifier` as your basic model without parameters set, and also pass it `params`.\n",
    "\n",
    "To speed things up, set `n_jobs=-1` to use all available CPU cores. Set `verbose=1` so you get updates as it proceeds - useful for making sure it is actually working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh2JFsl2hklc",
    "outputId": "3c96c67c-aa16-4e7a-d8ea-9ee49a1c2a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 44 candidates, totalling 220 fits\n",
      "The grid search with cross-validation took ----- 29.307926177978516 seconds ----- to fit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "params = dict(kernel=['linear', 'rbf', 'poly', 'sigmoid'], \n",
    "              C=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7, 2.0])\n",
    "\n",
    "classifier = SVC(random_state = 5)\n",
    "\n",
    "# Your code here...\n",
    "gcv = GridSearchCV(estimator=classifier, param_grid=params, n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gcv.fit(X_train, y_train)\n",
    "print(\"The grid search with cross-validation took ----- %s seconds ----- to fit\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AUn_Y9Shklc"
   },
   "source": [
    "## What was the best model?\n",
    "\n",
    "`GridSearchCV` evaluated each possible model using the accuracy metric.\n",
    "\n",
    "The best model is stored inside `gcv` as `best_estimator_`. Its score is in `gcv.best_score_` and the actual hyperparameters used are in `gcv.best_params_`.\n",
    "\n",
    "(The score here is not the score on the training set, but the average score across subsets of the training set.)\n",
    "\n",
    "Take a look at these and then evaluate the best model using the test set.\n",
    "\n",
    "How does it compare to the four models you trained before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VnoKOCJfhklc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator is: SVC(C=1.7, random_state=5)\n",
      "Its score is: 0.67375\n",
      "And its hyperparameters are {'C': 1.7, 'kernel': 'rbf'}\n",
      "Using the best estimator, we get a score of 0.687, which is almost equal to the best model of the previous four that also happens to use the radial basis function kernel and a C value of 1.\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "print(f\"The best estimator is: {gcv.best_estimator_}\\nIts score is: {gcv.best_score_}\\nAnd its hyperparameters are {gcv.best_params_}\")\n",
    "print(f\"Using the best estimator, we get a score of {gcv.score(X_test, y_test)}, which is almost equal to the best model of the previous four that also happens to use the radial basis function kernel and a C value of 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXg8JK5Thklc"
   },
   "source": [
    "## What is easier to classify? Democrat or Republican?\n",
    "\n",
    "Accuracy only gives one impression. We have three classes here, so print a classification report for each of the baseline models.\n",
    "\n",
    "`sklearn.metrics.classification_report` takes two arguments: the true labels and a model's predictions.\n",
    "\n",
    "You can get predictions for `X_test` by using the `.predict()` method of a trained model.\n",
    "\n",
    "How does the best model do at predicting the two classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5gMutKv5hklc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.69      0.67      0.68       497\n",
      "  Republican       0.68      0.70      0.69       503\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.69      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n",
      "Let's go through some of the metrics of the report. Accuracy tells us the rate of how much the model predicts correctly, including all classes.\n",
      "Since accuracy takes all classes into account, we can't know how well our model classifies each class separately.\n",
      "\n",
      "That's where f1-score comes in, as it tells us how well is the model at predicting each class taking into account its precision and recall.\n",
      "\n",
      "If the f1-score of predicting a democrat class is lower than the f1-score of predicting a republican class, then we can say that with this model and with this dataset it's easier to classify republican cases, and viceversa.\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_predict = gcv.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(\"Let's go through some of the metrics of the report. Accuracy tells us the rate of how much the model predicts correctly, including all classes.\")\n",
    "print(\"Since accuracy takes all classes into account, we can't know how well our model classifies each class separately.\\n\")\n",
    "print(\"That's where f1-score comes in, as it tells us how well is the model at predicting each class taking into account its precision and recall.\\n\")\n",
    "print(\"If the f1-score of predicting a democrat class is lower than the f1-score of predicting a republican class, then we can say that with this model and with this dataset it's easier to classify republican cases, and viceversa.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
